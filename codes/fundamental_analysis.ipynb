{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7699a4",
   "metadata": {},
   "source": [
    "## Data extraction and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea47d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, json, requests, time, os, os.path, math, urllib\n",
    "from sys import stdout\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from pandas_datareader.data import get_data_yahoo\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb44a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns python object representation of JSON in response\n",
    "def get_response(symbol, older_than, retries=5):\n",
    "    url = 'https://api.stocktwits.com/api/2/streams/symbol/%s.json?max=%d' % (symbol, older_than-1)\n",
    "    for _ in range(retries):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return json.loads(response.content)\n",
    "        elif response.status_code == 429:\n",
    "            print(response.content)\n",
    "            return None\n",
    "        time.sleep(1.0)\n",
    "    # couldn't get response\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af40e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extends the current dataset for a given symbol with more tweets\n",
    "def get_older_tweets(symbol, num_queries):    \n",
    "    path = './data/%s.json' % symbol\n",
    "    if os.path.exists(path):\n",
    "        # extending an existing json file\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if len(data) > 0:\n",
    "                older_than = data[-1]['id']\n",
    "            else:\n",
    "                older_than = 1000000000000\n",
    "    else:\n",
    "        # creating a new json file\n",
    "        data = []\n",
    "        older_than = 1000000000000 # any huge number\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        content = get_response(symbol, older_than)\n",
    "        if content == None:\n",
    "            print('Error, an API query timed out')\n",
    "            break\n",
    "        data.extend(content['messages'])\n",
    "        older_than = data[-1]['id']\n",
    "        stdout.write('\\rSuccessfully made query %d' % (i+1))\n",
    "        stdout.flush()\n",
    "        # sleep to make sure we don't get throttled\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    # write the new data to the JSON file\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ad45aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tweets for symbol TATAMOTORS.NSE\n",
      "Successfully made query 100Done\n"
     ]
    }
   ],
   "source": [
    "# get some data\n",
    "# apparently a client can only make 200 requests an hour, so we can't get all the data at once\n",
    "\n",
    "# make data directory if needed\n",
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "symbols = symbols = ['TATAMOTORS.NSE']\n",
    "tweets_per_symbol = 3000\n",
    "for symbol in symbols:\n",
    "    path = './data/%s.json' % symbol\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            num_tweets = len(json.load(f))\n",
    "    else:\n",
    "        num_tweets = 0\n",
    "    num_queries = (tweets_per_symbol - num_tweets - 1)/30 + 1\n",
    "    num_queries = int(num_queries)\n",
    "    if num_queries > 0:\n",
    "        print('Getting tweets for symbol %s'% symbol)\n",
    "        get_older_tweets(symbol, num_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c35c82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# check that we're doing the querying and appending correctly without getting duplicates\n",
    "# and that message IDs are in descending order\n",
    "symbol = 'TATAMOTORS.NSE'\n",
    "with open('./data/%s.json' % symbol, 'r') as f:\n",
    "    data = json.load(f)\n",
    "S = set()\n",
    "old_id = 1000000000000\n",
    "for message in data:\n",
    "    message_id = message['id']\n",
    "    assert message_id not in S\n",
    "    assert message_id < old_id\n",
    "    old_id = message_id\n",
    "    S.add(message_id)\n",
    "print('Passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a96ddb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes in a JSON and returns a Pandas DataFrame for easier operation. \n",
    "def stocktwits_json_to_df(data, verbose=False):\n",
    "    #data = json.loads(results)\n",
    "    columns = ['id','created_at','username','name','user_id','body','basic_sentiment','reshare_count']\n",
    "    db = pd.DataFrame(index=range(len(data)),columns=columns)\n",
    "    for i, message in enumerate(data):\n",
    "        db.loc[i,'id'] = message['id']\n",
    "        db.loc[i,'created_at'] = message['created_at']\n",
    "        db.loc[i,'username'] = message['user']['username']\n",
    "        db.loc[i,'name'] = message['user']['name']\n",
    "        db.loc[i,'user_id'] = message['user']['id']\n",
    "        db.loc[i,'body'] = message['body']\n",
    "        #We'll classify bullish as +1 and bearish as -1 to make it ready for classification training\n",
    "        try:\n",
    "            if (message['entities']['sentiment']['basic'] == 'Bullish'):\n",
    "                db.loc[i,'basic_sentiment'] = 1\n",
    "            elif (message['entities']['sentiment']['basic'] == 'Bearish'):\n",
    "                db.loc[i,'basic_sentiment'] = -1\n",
    "            else:\n",
    "                db.loc[i,'basic_sentiment'] = 0\n",
    "        except:\n",
    "                db.loc[i,'basic_sentiment'] = 0\n",
    "        db.loc[i,'reshare_count'] = message['reshares']['reshared_count']\n",
    "        for j, symbol in enumerate(message['symbols']):\n",
    "                db.loc[i,'symbol'+str(j)] = symbol['symbol']\n",
    "        if verbose:\n",
    "            #print message\n",
    "            print(db.loc[i,:])\n",
    "    db['created_at'] = pd.to_datetime(db['created_at'])\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "354d7c11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (690928719.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[33], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    path = \"C:\\Users\\nanda\\PISL project\\data\\TATAMOTORS.NSE.json\" % filename\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Load tweets for visualizing data\n",
    "filename = 'TATAMOTORS.NSE'\n",
    "path = \"C:\\Users\\nanda\\PISL project\\data\\TATAMOTORS.NSE.json\" % filename\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "db = stocktwits_json_to_df(data)\n",
    "print('%d examples extracted ' % db.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6aa89e40",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2877805897.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    with open('C:\\Users\\nanda\\PISL project\\data\\TATAMOTORS.NSE.json') as json_file:\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "with open('C:\\Users\\nanda\\PISL project\\data\\TATAMOTORS.NSE.json') as json_file:\n",
    "\tjsondata = json.load(json_file)\n",
    "\n",
    "data_file = open('C:\\Users\\nanda\\PISL project\\data\\TATAMOTORS.NSE.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(data_file)\n",
    "\n",
    "count = 0\n",
    "for data in jsondata:\n",
    "\tif count == 0:\n",
    "\t\theader = data.keys()\n",
    "\t\tcsv_writer.writerow(header)\n",
    "\t\tcount += 1\n",
    "\tcsv_writer.writerow(data.values())\n",
    "\n",
    "data_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f9abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
